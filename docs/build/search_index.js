var documenterSearchIndex = {"docs":
[{"location":"api.html#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api.html#Main-Functions","page":"API Reference","title":"Main Functions","text":"","category":"section"},{"location":"api.html#Model-Types","page":"API Reference","title":"Model Types","text":"","category":"section"},{"location":"api.html#IV-Estimators","page":"API Reference","title":"IV Estimators","text":"","category":"section"},{"location":"api.html#Variance-Specification","page":"API Reference","title":"Variance Specification","text":"","category":"section"},{"location":"api.html#StatsAPI-Interface","page":"API Reference","title":"StatsAPI Interface","text":"All fitted models (OLSEstimator, IVEstimator) implement the StatsAPI interface:","category":"section"},{"location":"api.html#Coefficients-and-Inference","page":"API Reference","title":"Coefficients and Inference","text":"Function Description\ncoef(model) Coefficient estimates\ncoefnames(model) Coefficient names\nstderror(model) Standard errors (default HC1)\nstderror(vcov_type, model) Standard errors with specific vcov\nvcov(model) Variance-covariance matrix\nvcov(vcov_type, model) Vcov with specific estimator\nconfint(model) 95% confidence intervals\ncoeftable(model) Full coefficient table","category":"section"},{"location":"api.html#Model-Statistics","page":"API Reference","title":"Model Statistics","text":"Function Description\nnobs(model) Number of observations\ndof(model) Degrees of freedom (model)\ndof_residual(model) Degrees of freedom (residual)\nr2(model) R-squared\nadjr2(model) Adjusted R-squared\ndeviance(model) Residual sum of squares\nnulldeviance(model) Null model deviance\nloglikelihood(model) Log-likelihood\nnullloglikelihood(model) Null model log-likelihood","category":"section"},{"location":"api.html#Prediction-and-Residuals","page":"API Reference","title":"Prediction and Residuals","text":"Function Description\nfitted(model) Fitted values (if saved)\nresiduals(model) Residuals (if saved)\npredict(model, newdata) Predict for new data\nresponsename(model) Name of response variable","category":"section"},{"location":"api.html#Model-Fields","page":"API Reference","title":"Model Fields","text":"","category":"section"},{"location":"api.html#OLSEstimator-Fields","page":"API Reference","title":"OLSEstimator Fields","text":"Field Type Description\ncoef Vector{T} Coefficient estimates\nvcov_matrix Matrix{T} Variance-covariance matrix\nF T Robust Wald F-statistic\np T p-value of F-statistic\nesample BitVector Estimation sample indicator\nnobs Int Number of observations\ndof Int Model degrees of freedom\ndof_residual Int Residual degrees of freedom\ndof_fes Int Fixed effects degrees of freedom\nrss T Residual sum of squares\ntss T Total sum of squares\nr2 T R-squared\nr2_within T Within R-squared (if FE)\nadjr2 T Adjusted R-squared\nconverged Bool Convergence status\niterations Int Number of iterations","category":"section"},{"location":"api.html#IVEstimator-Fields","page":"API Reference","title":"IVEstimator Fields","text":"All fields from OLSEstimator, plus:\n\nField Type Description\nF_kp T Joint first-stage F-statistic\np_kp T p-value of joint F\nF_kp_per_endo Vector{T} Per-endogenous F-statistics\np_kp_per_endo Vector{T} Per-endogenous p-values","category":"section"},{"location":"api.html#Variance-Estimators","page":"API Reference","title":"Variance Estimators","text":"","category":"section"},{"location":"api.html#Heteroskedasticity-Robust","page":"API Reference","title":"Heteroskedasticity-Robust","text":"From CovarianceMatrices.jl:\n\nHC0()   # White's estimator\nHC1()   # DOF-adjusted (default)\nHC2()   # Leverage-adjusted\nHC3()   # Jackknife-like\nHC4()   # Cribari-Neto\nHC5()   # Cribari-Neto modified","category":"section"},{"location":"api.html#Cluster-Robust","page":"API Reference","title":"Cluster-Robust","text":"CR0(:cluster)           # No adjustment\nCR1(:cluster)           # G/(G-1) adjustment\nCR2(:cluster)           # Bell-McCaffrey\nCR3(:cluster)           # Squared leverage\n\n# Multi-way\nCR1(:cluster1, :cluster2)","category":"section"},{"location":"api.html#HAC","page":"API Reference","title":"HAC","text":"Bartlett(bandwidth)\nParzen(bandwidth)\nQuadraticSpectral(bandwidth)\nTukeyHanning(bandwidth)\nTruncated(bandwidth)","category":"section"},{"location":"api.html#Index","page":"API Reference","title":"Index","text":"","category":"section"},{"location":"api.html#Regress.ols","page":"API Reference","title":"Regress.ols","text":"ols(df, formula; kwargs...) -> OLSEstimator\n\nEstimate a linear model using Ordinary Least Squares (OLS).\n\nSupports high-dimensional categorical variables (fixed effects) but not instrumental variables. For IV models, use iv(estimator, df, formula).\n\nArguments\n\ndf: a Table (e.g., DataFrame)\nformula: A formula created using @formula(y ~ x1 + x2 + fe(group))\n\nKeyword Arguments\n\ncontrasts::Dict = Dict(): Contrast codings for categorical variables\nweights::Union{Nothing, Symbol}: Column name for weights\nsave::Symbol = :residuals: Save residuals (:residuals), fixed effects (:fe), or both (:all)\nsave_cluster::Union{Symbol, Vector{Symbol}, Nothing}: Cluster variables to save for post-estimation vcov\ndof_add::Integer = 0: Manual adjustment to degrees of freedom\nmethod::Symbol = :cpu: Computation method (:cpu, :CUDA, :Metal)\nnthreads::Integer: Number of threads (default: Threads.nthreads() for CPU)\ndouble_precision::Bool = true for CPU, false otherwise: Use Float64 vs Float32\ntol::Real = 1e-6: Tolerance for fixed effects demeaning\nmaxiter::Integer = 10000: Maximum iterations for fixed effects\ndrop_singletons::Bool = true: Drop singleton observations\nprogress_bar::Bool = true: Show progress bar during estimation\nsubset::Union{Nothing, AbstractVector}: Select specific rows\n\nReturns\n\nOLSEstimator{T}: Fitted model (T is Float64 or Float32 depending on double_precision)\n\nExamples\n\nusing DataFrames, RDatasets, Regress\n\ndf = dataset(\"plm\", \"Cigar\")\n\n# Simple OLS\nmodel = ols(df, @formula(Sales ~ NDI + Pop))\n\n# With fixed effects\nmodel = ols(df, @formula(Sales ~ NDI + fe(State) + fe(Year)))\n\n# Post-estimation robust standard errors\nvcov(HC3(), model)\nvcov(:State, :CR1, model)  # cluster-robust\n\n# With weights\nmodel = ols(df, @formula(Sales ~ NDI), weights = :Pop)\n\nPost-Estimation Variance Calculations\n\nAfter fitting a model, you can compute different variance-covariance matrices without re-running the regression:\n\nmodel = ols(df, @formula(y ~ x1 + x2 + fe(firm_id)))\n\n# Different robust estimators\nvcov(HC3(), model)           # Heteroskedasticity-robust (HC3)\nvcov(HC1(), model)           # Default (HC1)\n\n# Cluster-robust (using stored cluster variable from fe())\nvcov(:firm_id, :CR1, model)\n\n# Two-way clustering\nvcov((:firm_id, :year), :CR1, model)\n\n# HAC (time series)\nvcov(Bartlett(5), model)\n\n# Standard errors and coefficient table\nstderror(HC3(), model)\ncoeftable(model, :firm_id, :CR1)\n\nSee also: iv, OLSEstimator\n\n\n\n\n\n","category":"function"},{"location":"api.html#Regress.iv","page":"API Reference","title":"Regress.iv","text":"iv(estimator::AbstractIVEstimator, df, formula; kwargs...) -> IVEstimator\n\nEstimate an instrumental variables model using the specified estimator.\n\nArguments\n\nestimator::AbstractIVEstimator: Estimator type (TSLS(), LIML(), Fuller(), KClass())\ndf: a Table (e.g., DataFrame)\nformula: A formula with IV syntax: @formula(y ~ x + (endo ~ instrument))\n\nKeyword Arguments\n\nSame as ols(), plus:\n\nfirst_stage::Bool = true: Compute first-stage F-statistics\n\nReturns\n\nIVEstimator{T}: Fitted IV model (T is Float64 or Float32 depending on double_precision)\n\nAvailable Estimators\n\nTSLS(): Two-Stage Least Squares (k = 1)\nLIML(): Limited Information Maximum Likelihood (k = k_LIML)\nFuller(a): Fuller bias-corrected estimator (k = k_LIML - a/(n-L-p)), default a=1\nKClass(k): Generic K-class estimator with user-specified k\n\nExamples\n\nusing DataFrames, RDatasets, Regress\n\ndf = dataset(\"plm\", \"Cigar\")\n\n# Two-stage least squares\nmodel = iv(TSLS(), df, @formula(Sales ~ NDI + (Price ~ Pimin)))\n\n# LIML (better with weak instruments)\nmodel = iv(LIML(), df, @formula(Sales ~ NDI + (Price ~ Pimin)))\n\n# Fuller bias-corrected (approximately median-unbiased)\nmodel = iv(Fuller(), df, @formula(Sales ~ NDI + (Price ~ Pimin)))\nmodel = iv(Fuller(4.0), df, @formula(Sales ~ NDI + (Price ~ Pimin)))  # minimize MSE\n\n# Generic K-class\nmodel = iv(KClass(0.5), df, @formula(Sales ~ NDI + (Price ~ Pimin)))\n\n# With fixed effects\nmodel = iv(TSLS(), df, @formula(Sales ~ (Price ~ Pimin) + fe(State)))\n\n# Post-estimation\nvcov(HC3(), model)\ncoeftable(model)\n\nPost-Estimation Variance Calculations\n\nIV models support the same post-estimation vcov calculations as OLS:\n\nmodel = iv(TSLS(), df, @formula(y ~ x + (endo ~ instrument)))\n\n# Heteroskedasticity-robust\nvcov(HC3(), model)\n\n# Update model with different vcov\nmodel_hc3 = model + vcov(HC3())\n\n# Cluster-robust (save cluster variable when fitting)\nmodel = iv(TSLS(), df, @formula(y ~ x + (endo ~ inst)), save_cluster = :firm_id)\nvcov(CR1(:firm_id), model)\n\n# Two-way clustering\nmodel = iv(TSLS(), df, @formula(...), save_cluster = [:firm_id, :year])\nvcov(CR1(:firm_id, :year), model)\n\nSee also: ols, TSLS, LIML, Fuller, KClass, IVEstimator\n\n\n\n\n\n","category":"function"},{"location":"api.html#Regress.fe","page":"API Reference","title":"Regress.fe","text":"fe(m::OLSEstimator; keepkeys = false)\n\nReturn a DataFrame with fixed effects estimates. The output is aligned with the original DataFrame used in ols.\n\nKeyword arguments\n\nkeepkeys::Bool : Should the returned DataFrame include the original variables used to define groups? Default to false\n\n\n\n\n\nfe(m::IVEstimator; keepkeys = false)\n\nReturn a DataFrame with fixed effects estimates.\n\n\n\n\n\n","category":"function"},{"location":"api.html#Regress.partial_out","page":"API Reference","title":"Regress.partial_out","text":"Partial out variables in a Dataframe\n\nArguments\n\ndf: A table\nformula::FormulaTerm: A formula created using @formula\nadd_mean::Bool: Should the initial mean added to the returned variable?\nmethod::Symbol: A symbol for the method. Default is :cpu. Alternatively,  :gpu requires CuArrays. In this case, use the option double_precision = false to use Float32.\nmaxiter::Integer: Maximum number of iterations\ndouble_precision::Bool: Should the demeaning operation use Float64 rather than Float32? Default to true.\ntol::Real: Tolerance\nalign::Bool: Should the returned DataFrame align with the original DataFrame in case of missing values? Default to true.\n\nReturns\n\n::DataFrame: a dataframe with as many columns as there are dependent variables and as many rows as the original dataframe.\n::Vector{Int}: a vector of iterations for each column\n::Vector{Bool}: a vector of success for each column\n\nDetails\n\npartial_out returns the residuals of a set of variables after regressing them on a set of regressors. The syntax is similar to reg - but it accepts multiple dependent variables. It returns a dataframe with as many columns as there are dependent variables and as many rows as the original dataframe. The regression model is estimated on only the rows where none of the dependent variables is missing.  Finally, with the option add_mean = true, the mean of the initial variable is added to the residuals.\n\nExamples\n\nusing  RDatasets, DataFrames, MetricsOLSEstimators, Gadfly\ndf = dataset(\"datasets\", \"iris\")\nresult = partial_out(df, @formula(SepalWidth + SepalLength ~ fe(Species)), add_mean = true)\nplot(layer(result[1], x=\"SepalWidth\", y=\"SepalLength\", Stat.binmean(n=10), Geom.point),\n   layer(result[1], x=\"SepalWidth\", y=\"SepalLength\", Geom.smooth(method=:lm)))\n\n\n\n\n\n","category":"function"},{"location":"api.html#Regress.first_stage","page":"API Reference","title":"Regress.first_stage","text":"first_stage(m::IVEstimator) -> FirstStageResult\n\nReturn first-stage diagnostics for an IV model. Uses the variance estimator stored in the model.\n\nReturns\n\nA FirstStageResult struct containing:\n\nJoint Kleibergen-Paap F-statistic and p-value\nPer-endogenous F-statistics and p-values\nMetadata (number of instruments, variance estimator type)\n\nExamples\n\nmodel = iv(TSLS(), df, @formula(y ~ x + (endo ~ z1 + z2)))\nfs = first_stage(model)\nfs.F_joint           # Joint F-statistic\nfs.F_per_endo        # Per-endogenous F-stats\n\nSee also: FirstStageResult\n\n\n\n\n\n","category":"function"},{"location":"api.html#Regress.OLSEstimator","page":"API Reference","title":"Regress.OLSEstimator","text":"OLSEstimator <: StatsAPI.RegressionModel\n\nModel type for linear regression estimated by ordinary least squares (OLS).\n\nUses a GLM.jl-compatible structure with separate response (rr) and predictor (pp) components, plus econometrics-specific fixed effects (fes).\n\nUse ols(df, formula) to fit this model type.\n\nType Parameters\n\nT: Float type (Float64 or Float32)\nP: Predictor type (OLSPredictorChol or OLSPredictorQR)\nV: Variance-covariance estimator type (HC1, HC3, CR1, etc.)\n\nFields\n\nrr::OLSResponse{T}: Response object (y, fitted values, weights)\npp::OLSLinearPredictor{T}: Predictor object (X, coefficients, factorization)\nfes::OLSFixedEffects{T}: Fixed effects component\nformula::FormulaTerm: Original formula\nformula_schema::FormulaTerm: Schema for predict\ncontrasts::Dict: Contrasts used\nesample::BitVector: Estimation sample indicator\ncoefnames::Vector{String}: Coefficient names\nbasis_coef::BitVector: Non-collinear coefficients indicator\nnobs::Int: Number of observations\ndof::Int: Degrees of freedom (parameters estimated)\ndof_fes::Int: DOF absorbed by fixed effects\ndof_residual::Int: Residual degrees of freedom\ntss_total::T: Total sum of squares (before FE)\ntss_partial::T: TSS after partialing out FEs\nrss::T: Residual sum of squares\nr2::T: R-squared\nr2_within::T: Within R-squared (with FEs)\nhas_intercept::Bool: Whether model has intercept\nvcov_estimator::V: Variance-covariance estimator (deep copy)\nvcov_matrix::Symmetric{T, Matrix{T}}: Precomputed variance-covariance matrix\nse::Vector{T}: Standard errors\nt_stats::Vector{T}: t-statistics\np_values::Vector{T}: p-values\nF::T: F-statistic (computed with vcov_estimator)\np::T: P-value of F-statistic\n\n\n\n\n\n","category":"type"},{"location":"api.html#Regress.IVEstimator","page":"API Reference","title":"Regress.IVEstimator","text":"IVEstimator <: StatsAPI.RegressionModel\n\nModel type for instrumental variables regression.\n\nUse iv(estimator, df, formula) to fit this model type, where estimator is one of TSLS(), LIML(), etc.\n\nType Parameters\n\nT: Float type (Float64 or Float32)\nV: Variance-covariance estimator type (HC1, HC3, CR1, etc.)\n\nExamples\n\niv(TSLS(), df, @formula(y ~ x + (endo ~ instrument)))\n\n\n\n\n\n","category":"type"},{"location":"api.html#Regress.FirstStageResult","page":"API Reference","title":"Regress.FirstStageResult","text":"FirstStageResult{T}\n\nContainer for first-stage regression diagnostics from IV estimation. Returned by first_stage(model).\n\nFields\n\nF_joint::T: Joint first-stage F-statistic (Kleibergen-Paap)\np_joint::T: p-value of joint F-statistic\nendogenous_names::Vector{String}: Names of endogenous variables\nF_per_endo::Vector{T}: F-statistic for each endogenous variable\np_per_endo::Vector{T}: p-value for each endogenous variable\nn_endogenous::Int: Number of endogenous variables\nn_instruments::Int: Number of excluded instruments\nvcov_type::String: Variance estimator used (e.g., \"HC1\", \"HC3\")\n\nExamples\n\nmodel = iv(TSLS(), df, @formula(y ~ x + (endo ~ z1 + z2)))\nfs = first_stage(model)\nfs.F_joint           # Joint F-statistic\nfs.F_per_endo        # Per-endogenous F-statistics\n\n\n\n\n\n","category":"type"},{"location":"api.html#Regress.AbstractIVEstimator","page":"API Reference","title":"Regress.AbstractIVEstimator","text":"AbstractIVEstimator\n\nAbstract type for instrumental variables estimators.\n\nConcrete subtypes include:\n\nTSLS: Two-Stage Least Squares (k = 1)\nLIML: Limited Information Maximum Likelihood (k = k_LIML)\nFuller: Fuller bias-corrected estimator (k = k_LIML - a/(n-L-p))\nKClass: Generic K-class estimator with user-specified k\n\nAll estimators are used via the iv() function:\n\niv(TSLS(), df, @formula(y ~ x + (endo ~ instrument)))\niv(LIML(), df, @formula(y ~ x + (endo ~ instrument)))\niv(Fuller(), df, @formula(y ~ x + (endo ~ instrument)))\niv(Fuller(4.0), df, @formula(y ~ x + (endo ~ instrument)))\niv(KClass(0.5), df, @formula(y ~ x + (endo ~ instrument)))\n\n\n\n\n\n","category":"type"},{"location":"api.html#Regress.TSLS","page":"API Reference","title":"Regress.TSLS","text":"TSLS <: AbstractIVEstimator\n\nTwo-Stage Least Squares (2SLS) estimator for instrumental variables models.\n\nThis is the most common IV estimator, which proceeds in two stages:\n\nFirst stage: Regress endogenous variables on instruments and exogenous variables\nSecond stage: Regress outcome on predicted endogenous variables and exogenous variables\n\nUsage\n\niv(TSLS(), df, @formula(y ~ x + (endo ~ instrument)))\n\n\n\n\n\n","category":"type"},{"location":"api.html#Regress.LIML","page":"API Reference","title":"Regress.LIML","text":"LIML <: AbstractIVEstimator\n\nLimited Information Maximum Likelihood estimator for instrumental variables models.\n\nLIML is an alternative to 2SLS that can have better finite-sample properties, especially when instruments are weak. It uses k = k_LIML, the minimum eigenvalue of the generalized eigenvalue problem M1 * v = k * M2 * v.\n\nUsage\n\niv(LIML(), df, @formula(y ~ x + (endo ~ instrument)))\n\nReferences\n\nAnderson, T. W. and Rubin, H. (1949). Estimation of the parameters of a single equation in a complete system of stochastic equations. Annals of Mathematical Statistics, 20(1):46-63.\n\n\n\n\n\n","category":"type"},{"location":"api.html#Regress.VcovSpec","page":"API Reference","title":"Regress.VcovSpec","text":"VcovSpec{V}\n\nWrapper type for variance estimator specification. Used with + operator: model + vcov(HC3())\n\nFields\n\nestimator::V: The variance estimator (HC0, CR1, etc.)\n\nExamples\n\nmodel = ols(df, @formula(y ~ x))\nmodel_hc3 = model + vcov(HC3())\nmodel_cr1 = model + vcov(CR1(:cluster))\n\n\n\n\n\n","category":"type"},{"location":"fixed_effects.html#Fixed-Effects","page":"Fixed Effects","title":"Fixed Effects","text":"","category":"section"},{"location":"fixed_effects.html#Overview","page":"Fixed Effects","title":"Overview","text":"Regress.jl efficiently handles high-dimensional fixed effects by absorbing (partialing out) categorical variables from the regression.","category":"section"},{"location":"fixed_effects.html#Specifying-Fixed-Effects","page":"Fixed Effects","title":"Specifying Fixed Effects","text":"Use fe() in the formula:\n\n# One-way fixed effects\nmodel = ols(df, @formula(y ~ x + fe(firm)))\n\n# Two-way fixed effects\nmodel = ols(df, @formula(y ~ x + fe(firm) + fe(year)))\n\n# Three-way (or more)\nmodel = ols(df, @formula(y ~ x + fe(firm) + fe(year) + fe(industry)))","category":"section"},{"location":"fixed_effects.html#Interacted-Fixed-Effects","page":"Fixed Effects","title":"Interacted Fixed Effects","text":"","category":"section"},{"location":"fixed_effects.html#Nested-Fixed-Effects","page":"Fixed Effects","title":"Nested Fixed Effects","text":"Combine multiple categoricals into a single fixed effect:\n\n# Firm-year fixed effects\nmodel = ols(df, @formula(y ~ x + fe(firm)&fe(year)))","category":"section"},{"location":"fixed_effects.html#Continuous-Interactions-(Slopes)","page":"Fixed Effects","title":"Continuous Interactions (Slopes)","text":"Allow for heterogeneous slopes:\n\n# Firm-specific time trends\nmodel = ols(df, @formula(y ~ x + fe(firm)&year))\n\n# Industry-specific effects of x\nmodel = ols(df, @formula(y ~ fe(industry)&x))","category":"section"},{"location":"fixed_effects.html#Full-Factorial","page":"Fixed Effects","title":"Full Factorial","text":"Use * for full factorial expansion:\n\n# Expands to: fe(firm) + fe(year) + fe(firm)&fe(year)\nmodel = ols(df, @formula(y ~ x + fe(firm)*fe(year)))","category":"section"},{"location":"fixed_effects.html#Algorithm","page":"Fixed Effects","title":"Algorithm","text":"Regress.jl uses an iterative demeaning algorithm from FixedEffects.jl:\n\nFor each fixed effect, compute group means\nSubtract group means from all variables\nRepeat until convergence","category":"section"},{"location":"fixed_effects.html#Convergence-Options","page":"Fixed Effects","title":"Convergence Options","text":"model = ols(df, @formula(y ~ x + fe(firm)),\n    tol = 1e-8,        # Convergence tolerance (default 1e-6)\n    maxiter = 50000    # Maximum iterations (default 10000)\n)","category":"section"},{"location":"fixed_effects.html#Checking-Convergence","page":"Fixed Effects","title":"Checking Convergence","text":"model.converged     # Boolean\nmodel.iterations    # Number of iterations","category":"section"},{"location":"fixed_effects.html#Singleton-Groups","page":"Fixed Effects","title":"Singleton Groups","text":"By default, observations in groups with only one observation are dropped:\n\nmodel = ols(df, @formula(y ~ x + fe(firm)),\n    drop_singletons = true   # Default\n)\n\nDisable to keep singletons:\n\nmodel = ols(df, @formula(y ~ x + fe(firm)),\n    drop_singletons = false\n)","category":"section"},{"location":"fixed_effects.html#Within-R-squared","page":"Fixed Effects","title":"Within R-squared","text":"For models with fixed effects, two R-squared values are reported:\n\nr2(model): Overall R-squared (including fixed effects)\nmodel.r2_within: Within R-squared (variation explained by x only)","category":"section"},{"location":"fixed_effects.html#Degrees-of-Freedom","page":"Fixed Effects","title":"Degrees of Freedom","text":"Fixed effects consume degrees of freedom. Check:\n\ndof_residual(model)  # n - k - number of FE groups\nmodel.dof_fes        # Degrees of freedom absorbed by FE","category":"section"},{"location":"fixed_effects.html#GPU-Acceleration","page":"Fixed Effects","title":"GPU Acceleration","text":"For very large datasets, enable GPU acceleration:\n\n# CUDA (NVIDIA GPUs)\nmodel = ols(df, @formula(y ~ x + fe(firm)),\n    method = :CUDA\n)\n\n# Metal (Apple Silicon)\nmodel = ols(df, @formula(y ~ x + fe(firm)),\n    method = :Metal\n)\n\nGPU acceleration is beneficial for n > 10 million observations.","category":"section"},{"location":"fixed_effects.html#Multi-threading","page":"Fixed Effects","title":"Multi-threading","text":"Enable multi-threading for CPU computation:\n\nmodel = ols(df, @formula(y ~ x + fe(firm)),\n    method = :cpu,\n    nthreads = 8   # Number of threads\n)","category":"section"},{"location":"fixed_effects.html#Extracting-Fixed-Effects","page":"Fixed Effects","title":"Extracting Fixed Effects","text":"To recover the fixed effect estimates:\n\n# Save fixed effects during estimation\nmodel = ols(df, @formula(y ~ x + fe(firm)),\n    save = :fe   # or :all for both residuals and FE\n)\n\n# Access via fes component\nmodel.fes.values  # Named tuple of fixed effect values","category":"section"},{"location":"fixed_effects.html#Partial-Out","page":"Fixed Effects","title":"Partial Out","text":"Use partial_out() to remove fixed effects from variables without running a regression:\n\n# Partial out firm effects from y and x\nresult, converged = partial_out(df, @formula(y + x ~ fe(firm)))","category":"section"},{"location":"fixed_effects.html#Example","page":"Fixed Effects","title":"Example","text":"using Regress, DataFrames\n\n# Create panel data\nn_firms = 100\nn_years = 10\nn = n_firms * n_years\n\ndf = DataFrame(\n    firm = repeat(1:n_firms, inner = n_years),\n    year = repeat(2010:2019, outer = n_firms),\n    y = randn(n),\n    x = randn(n)\n)\n\n# One-way FE\nm1 = ols(df, @formula(y ~ x + fe(firm)))\nprintln(\"Within R2: \", m1.r2_within)\n\n# Two-way FE\nm2 = ols(df, @formula(y ~ x + fe(firm) + fe(year)))\nprintln(\"Within R2: \", m2.r2_within)\n\n# Firm-specific trends\nm3 = ols(df, @formula(y ~ x + fe(firm)&year))\n\n# Cluster-robust SE\nm2_cr = ols(df, @formula(y ~ x + fe(firm) + fe(year)),\n    save_cluster = :firm)\nm2_cr1 = m2_cr + vcov(CR1(:firm))\ncoeftable(m2_cr1)","category":"section"},{"location":"getting_started.html#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting_started.html#Installation","page":"Getting Started","title":"Installation","text":"Regress.jl can be installed from GitHub:\n\nusing Pkg\nPkg.add(url=\"https://github.com/gragusa/Regress.jl\")","category":"section"},{"location":"getting_started.html#Basic-Usage","page":"Getting Started","title":"Basic Usage","text":"","category":"section"},{"location":"getting_started.html#Loading-the-Package","page":"Getting Started","title":"Loading the Package","text":"using Regress, DataFrames\n\nRegress.jl re-exports StatsModels.jl for formula syntax and CovarianceMatrices.jl for variance estimators.","category":"section"},{"location":"getting_started.html#Creating-a-Model","page":"Getting Started","title":"Creating a Model","text":"Use ols() for Ordinary Least Squares:\n\ndf = DataFrame(y = randn(100), x = randn(100))\nmodel = ols(df, @formula(y ~ x))\n\nUse iv() for Instrumental Variables:\n\nmodel = iv(TSLS(), df, @formula(y ~ (endo ~ instrument)))","category":"section"},{"location":"getting_started.html#Accessing-Results","page":"Getting Started","title":"Accessing Results","text":"All standard StatsAPI methods work:\n\ncoef(model)          # Coefficient estimates\nstderror(model)      # Standard errors\nvcov(model)          # Variance-covariance matrix\nconfint(model)       # Confidence intervals\ncoeftable(model)     # Full coefficient table\nnobs(model)          # Number of observations\nr2(model)            # R-squared","category":"section"},{"location":"getting_started.html#Robust-Standard-Errors","page":"Getting Started","title":"Robust Standard Errors","text":"Update the variance estimator using + vcov(...):\n\nmodel_hc3 = model + vcov(HC3())\nstderror(model_hc3)  # HC3 standard errors\n\nOr compute directly:\n\nvcov(HC3(), model)\nstderror(HC3(), model)","category":"section"},{"location":"getting_started.html#Formula-Syntax","page":"Getting Started","title":"Formula Syntax","text":"","category":"section"},{"location":"getting_started.html#Basic-Terms","page":"Getting Started","title":"Basic Terms","text":"@formula(y ~ x1 + x2)           # Multiple regressors\n@formula(y ~ x1 + x2 + x1&x2)   # Interaction\n@formula(y ~ x1 * x2)           # Full factorial (x1 + x2 + x1&x2)","category":"section"},{"location":"getting_started.html#Fixed-Effects","page":"Getting Started","title":"Fixed Effects","text":"Use fe() to absorb high-dimensional categorical variables:\n\n@formula(y ~ x + fe(firm))                    # One-way FE\n@formula(y ~ x + fe(firm) + fe(year))         # Two-way FE\n@formula(y ~ x + fe(firm)&fe(year))           # Firm-year FE\n@formula(y ~ x + fe(firm)&year)               # Firm-specific time trends","category":"section"},{"location":"getting_started.html#Instrumental-Variables","page":"Getting Started","title":"Instrumental Variables","text":"Use parentheses (endo ~ instruments) for IV specification:\n\n@formula(y ~ x + (endo ~ z1 + z2))              # One endogenous variable\n@formula(y ~ x + (endo1 + endo2 ~ z1 + z2 + z3)) # Multiple endogenous\n@formula(y ~ (endo ~ z) + fe(firm))             # IV with fixed effects","category":"section"},{"location":"getting_started.html#Common-Options","page":"Getting Started","title":"Common Options","text":"","category":"section"},{"location":"getting_started.html#Keyword-Arguments-for-ols()-and-iv()","page":"Getting Started","title":"Keyword Arguments for ols() and iv()","text":"Argument Type Description\nweights Symbol Column for weighted regression\nsave_cluster Symbol or Tuple{Symbol,...} Save cluster variables for post-estimation\nsubset AbstractVector Boolean vector for subsetting\ncontrasts Dict Contrast codings for categorical variables\nmethod Symbol :cpu, :CUDA, or :Metal\nnthreads Integer Number of threads for fixed effects\ndouble_precision Bool Use Float64 (default true for CPU)\ntol Real Tolerance for FE demeaning (default 1e-6)\ndrop_singletons Bool Drop singleton groups (default true)","category":"section"},{"location":"getting_started.html#Example-with-Options","page":"Getting Started","title":"Example with Options","text":"model = ols(df, @formula(y ~ x + fe(firm)),\n    weights = :pop,\n    save_cluster = :firm,\n    method = :cpu,\n    nthreads = 4\n)","category":"section"},{"location":"iv.html#IV-Estimation","page":"IV Estimation","title":"IV Estimation","text":"","category":"section"},{"location":"iv.html#Two-Stage-Least-Squares","page":"IV Estimation","title":"Two-Stage Least Squares","text":"The iv() function estimates instrumental variables models:\n\nmodel = iv(TSLS(), df, @formula(y ~ x + (endo ~ z1 + z2)))","category":"section"},{"location":"iv.html#Formula-Syntax","page":"IV Estimation","title":"Formula Syntax","text":"Use parentheses to specify the endogenous variable and its instruments:\n\n# Single endogenous variable\n@formula(y ~ x + (endo ~ z1 + z2))\n\n# Multiple endogenous variables\n@formula(y ~ x + (endo1 + endo2 ~ z1 + z2 + z3))\n\n# With fixed effects\n@formula(y ~ x + (endo ~ z) + fe(firm))","category":"section"},{"location":"iv.html#Return-Type","page":"IV Estimation","title":"Return Type","text":"Returns an IVEstimator{T, V} where:\n\nT is the numeric type (Float64 or Float32)\nV is the variance estimator type","category":"section"},{"location":"iv.html#First-Stage-Diagnostics","page":"IV Estimation","title":"First-Stage Diagnostics","text":"Regress.jl automatically computes first-stage diagnostics for weak instrument detection.","category":"section"},{"location":"iv.html#Joint-F-Statistic","page":"IV Estimation","title":"Joint F-Statistic","text":"The Kleibergen-Paap rk Wald F-statistic tests all first-stage coefficients jointly:\n\nmodel = iv(TSLS(), df, @formula(y ~ x + (endo ~ z1 + z2)))\n\nmodel.F_kp    # Joint first-stage F-statistic\nmodel.p_kp    # p-value","category":"section"},{"location":"iv.html#Per-Endogenous-F-Statistics","page":"IV Estimation","title":"Per-Endogenous F-Statistics","text":"For models with multiple endogenous variables, individual first-stage F-statistics are computed:\n\nmodel.F_kp_per_endo   # Vector of F-statistics\nmodel.p_kp_per_endo   # Vector of p-values\n\nThe output displays a table with weak instrument warnings (F < 10):\n\nFirst-Stage F-Statistics (per endogenous variable):\n------------------------------------------------------------\nEndogenous                         F-stat      P-value  Weak?\n------------------------------------------------------------\nPrice                             23.4567       0.0000\nNDI                                8.1234       0.0012   Yes\n------------------------------------------------------------\nNote: F < 10 may indicate weak instruments (Stock-Yogo)","category":"section"},{"location":"iv.html#first_stage()-Function","page":"IV Estimation","title":"first_stage() Function","text":"Extract first-stage diagnostics as a structured object:\n\nfs = first_stage(model)\n\nfs.F_joint        # Joint F-statistic\nfs.p_joint        # p-value\nfs.F_per_endo     # Per-endogenous F-stats\nfs.p_per_endo     # Per-endogenous p-values\nfs.vcov_type      # Variance estimator name\nfs.n_instruments  # Number of excluded instruments\nfs.n_endogenous   # Number of endogenous variables\n\nPretty print the diagnostics:\n\nshow(fs)","category":"section"},{"location":"iv.html#Updating-Variance-Estimators","page":"IV Estimation","title":"Updating Variance Estimators","text":"The + vcov(...) syntax works with IV models and recomputes all diagnostics:\n\nmodel = iv(TSLS(), df, @formula(y ~ x + (endo ~ z1 + z2)))\n\n# Update to HC3 - recomputes first-stage F-stats\nmodel_hc3 = model + vcov(HC3())\n\nmodel_hc3.F_kp           # First-stage F with HC3\nmodel_hc3.F_kp_per_endo  # Per-endogenous F-stats with HC3","category":"section"},{"location":"iv.html#Example","page":"IV Estimation","title":"Example","text":"using Regress, DataFrames\n\n# Simulate IV data\nn = 1000\nz1 = randn(n)\nz2 = randn(n)\nx = randn(n)\nendo = 0.5 * z1 + 0.5 * z2 + 0.3 * randn(n)  # Endogenous\ny = 1.0 + 2.0 * x + 3.0 * endo + randn(n)\n\ndf = DataFrame(y = y, x = x, endo = endo, z1 = z1, z2 = z2)\n\n# TSLS estimation\nmodel = iv(TSLS(), df, @formula(y ~ x + (endo ~ z1 + z2)))\n\n# View results\ncoeftable(model)\n\n# First-stage diagnostics\nfs = first_stage(model)\nprintln(\"First-stage F: \", fs.F_joint)\n\n# Robust standard errors\nmodel_hc3 = model + vcov(HC3())\ncoeftable(model_hc3)\n\n# First-stage with robust vcov\nfs_hc3 = first_stage(model_hc3)","category":"section"},{"location":"iv.html#Available-Estimators","page":"IV Estimation","title":"Available Estimators","text":"Estimator Description\nTSLS() Two-Stage Least Squares\nLIML() Limited Information Maximum Likelihood\nFuller(a) Fuller bias-corrected estimator (default a=1)\nKClass(κ) Generic K-class estimator with user-specified κ","category":"section"},{"location":"iv.html#K-Class-Estimators:-LIML,-Fuller,-KClass","page":"IV Estimation","title":"K-Class Estimators: LIML, Fuller, KClass","text":"K-class estimators are a family of IV estimators indexed by a parameter κ:\n\nκ = 0: OLS (ignores endogeneity)\nκ = 1: TSLS\nκ = κ_LIML: Limited Information Maximum Likelihood","category":"section"},{"location":"iv.html#LIML","page":"IV Estimation","title":"LIML","text":"LIML is asymptotically more efficient than TSLS and more robust to weak instruments:\n\nmodel = iv(LIML(), df, @formula(y ~ x + (endo ~ z1 + z2)))\n\n# Access the computed kappa\nmodel.postestimation.kappa  # Should be ≥ 1","category":"section"},{"location":"iv.html#Fuller","page":"IV Estimation","title":"Fuller","text":"Fuller's modification reduces small-sample bias by subtracting a correction:\n\n# Default a=1 (minimizes bias)\nmodel = iv(Fuller(), df, @formula(y ~ x + (endo ~ z1 + z2)))\n\n# Custom a parameter\nmodel = iv(Fuller(4.0), df, @formula(y ~ x + (endo ~ z1 + z2)))\n\nFuller(1) is approximately unbiased. Fuller(4) minimizes MSE in some scenarios.","category":"section"},{"location":"iv.html#KClass","page":"IV Estimation","title":"KClass","text":"For direct control over κ:\n\n# Equivalent to TSLS\nmodel = iv(KClass(1.0), df, @formula(y ~ x + (endo ~ z1 + z2)))","category":"section"},{"location":"iv.html#Variance-Covariance-for-K-Class","page":"IV Estimation","title":"Variance-Covariance for K-Class","text":"All robust variance estimators work with K-class:\n\n# Default (HC1)\nvcov(model)\n\n# Heteroskedasticity-robust\nvcov(HC3(), model)\n\n# Cluster-robust (requires save_cluster during fit)\nmodel = iv(LIML(), df, @formula(y ~ (endo ~ z1 + z2)), save_cluster=:firm)\nvcov(CR1(:firm), model)\n\n# Using + operator\nmodel_robust = model + vcov(HC3())\nmodel_clustered = model + vcov(CR1(:firm))","category":"section"},{"location":"iv.html#Example:-LIML-with-Fixed-Effects-and-Many-Instruments","page":"IV Estimation","title":"Example: LIML with Fixed Effects and Many Instruments","text":"A complete example with one endogenous variable, multiple instruments, and fixed effects:\n\nusing Regress, DataFrames, StableRNGs\n\n# Simulate panel data with endogeneity\nrng = StableRNG(123)\nn = 1000\nn_firms = 100\n\n# Firm fixed effects\nfirm_id = repeat(1:n_firms, inner=n ÷ n_firms)\nfirm_effect = randn(rng, n_firms)[firm_id]\n\n# Multiple instruments (e.g., Bartik-style shifters)\nz1 = randn(rng, n)  # Instrument 1\nz2 = randn(rng, n)  # Instrument 2\nz3 = randn(rng, n)  # Instrument 3\nz4 = randn(rng, n)  # Instrument 4\n\n# Exogenous controls\nx1 = randn(rng, n)\nx2 = randn(rng, n)\n\n# Endogenous variable (correlated with error)\ne = randn(rng, n)\nu = 0.5 .* e .+ randn(rng, n)\nendo = 0.3 .* z1 .+ 0.25 .* z2 .+ 0.2 .* z3 .+ 0.15 .* z4 .+ u\n\n# Outcome\ny = 2.0 .* endo .+ 1.0 .* x1 .+ 0.5 .* x2 .+ firm_effect .+ e\n\ndf = DataFrame(\n    y = y, endo = endo, x1 = x1, x2 = x2,\n    z1 = z1, z2 = z2, z3 = z3, z4 = z4,\n    firm_id = firm_id\n)\n\n# LIML with fixed effects and 4 instruments\nmodel_liml = iv(LIML(), df,\n    @formula(y ~ x1 + x2 + (endo ~ z1 + z2 + z3 + z4) + fe(firm_id)))\n\n# View coefficients\ncoeftable(model_liml)\n\n# Check LIML kappa (should be close to 1 with strong instruments)\nprintln(\"LIML κ = \", model_liml.postestimation.kappa)\n\n# Compare with TSLS\nmodel_tsls = iv(TSLS(), df,\n    @formula(y ~ x1 + x2 + (endo ~ z1 + z2 + z3 + z4) + fe(firm_id)))\n\n# Coefficients should be similar but not identical\nprintln(\"LIML coef: \", coef(model_liml))\nprintln(\"TSLS coef: \", coef(model_tsls))\n\n# Fuller for reduced bias\nmodel_fuller = iv(Fuller(), df,\n    @formula(y ~ x1 + x2 + (endo ~ z1 + z2 + z3 + z4) + fe(firm_id)))\n\n# With cluster-robust standard errors\nmodel_liml_cluster = iv(LIML(), df,\n    @formula(y ~ x1 + x2 + (endo ~ z1 + z2 + z3 + z4) + fe(firm_id)),\n    save_cluster = :firm_id)\n\nmodel_clustered = model_liml_cluster + vcov(CR1(:firm_id))\ncoeftable(model_clustered)","category":"section"},{"location":"iv.html#Comparing-Estimators","page":"IV Estimation","title":"Comparing Estimators","text":"# All three estimators on the same data\ndf = DataFrame(y=y, x=x, endo=endo, z1=z1, z2=z2, z3=z3)\n\nm_tsls = iv(TSLS(), df, @formula(y ~ x + (endo ~ z1 + z2 + z3)))\nm_liml = iv(LIML(), df, @formula(y ~ x + (endo ~ z1 + z2 + z3)))\nm_fuller = iv(Fuller(), df, @formula(y ~ x + (endo ~ z1 + z2 + z3)))\n\n# Compare point estimates\nprintln(\"TSLS:   \", coef(m_tsls))\nprintln(\"LIML:   \", coef(m_liml))\nprintln(\"Fuller: \", coef(m_fuller))\n\n# LIML and Fuller should be very close with strong instruments\n# Fuller's kappa is slightly smaller than LIML's\nprintln(\"LIML κ:   \", m_liml.postestimation.kappa)\nprintln(\"Fuller κ: \", m_fuller.postestimation.kappa)  # κ_LIML - 1/(n-L-p)","category":"section"},{"location":"iv.html#When-to-Use-Each-Estimator","page":"IV Estimation","title":"When to Use Each Estimator","text":"Scenario Recommended Estimator\nStrong instruments (F > 10) TSLS or LIML (similar results)\nModerate instruments (F ∈ [5, 10]) LIML or Fuller(1)\nMany instruments LIML (more robust)\nFinite-sample bias concerns Fuller(1) or Fuller(4)\nExact-identification TSLS = LIML (identical)","category":"section"},{"location":"iv.html#Combining-with-Fixed-Effects","page":"IV Estimation","title":"Combining with Fixed Effects","text":"IV estimation works seamlessly with fixed effects:\n\nmodel = iv(TSLS(), df, @formula(y ~ x + (endo ~ z) + fe(firm) + fe(year)))\n\nFixed effects are partialed out from all variables (y, x, endo, z) before IV estimation.","category":"section"},{"location":"index.html#Regress.jl","page":"Home","title":"Regress.jl","text":"Regress.jl is a Julia package for estimating linear regression models with high-dimensional fixed effects and instrumental variables.","category":"section"},{"location":"index.html#Features","page":"Home","title":"Features","text":"OLS Estimation with support for high-dimensional fixed effects\nInstrumental Variables estimation (Two-Stage Least Squares)\nRobust Variance Estimation via integration with CovarianceMatrices.jl\nHeteroskedasticity-robust (HC0-HC5)\nCluster-robust (CR0-CR3)\nHAC estimators (Bartlett, Parzen, etc.)\nGPU Acceleration for large-scale fixed effects (CUDA, Metal)\nConvenient API with model + vcov(...) syntax for updating variance estimators","category":"section"},{"location":"index.html#Installation","page":"Home","title":"Installation","text":"using Pkg\nPkg.add(url=\"https://github.com/gragusa/Regress.jl\")","category":"section"},{"location":"index.html#Quick-Example","page":"Home","title":"Quick Example","text":"using Regress, DataFrames\n\n# Create sample data\ndf = DataFrame(\n    y = randn(1000),\n    x1 = randn(1000),\n    x2 = randn(1000),\n    firm = rand(1:50, 1000),\n    year = rand(2010:2020, 1000)\n)\n\n# OLS estimation\nmodel = ols(df, @formula(y ~ x1 + x2))\n\n# OLS with fixed effects\nmodel_fe = ols(df, @formula(y ~ x1 + fe(firm) + fe(year)))\n\n# Robust standard errors\nmodel_hc3 = model + vcov(HC3())\nstderror(model_hc3)\n\n# Cluster-robust standard errors\nmodel_cr = ols(df, @formula(y ~ x1), save_cluster = :firm)\nmodel_cr1 = model_cr + vcov(CR1(:firm))","category":"section"},{"location":"index.html#Contents","page":"Home","title":"Contents","text":"Pages = [\"getting_started.md\", \"ols.md\", \"iv.md\", \"fixed_effects.md\", \"variance.md\", \"api.md\"]\nDepth = 2","category":"section"},{"location":"index.html#Credits","page":"Home","title":"Credits","text":"This package is a fork of FixedEffectModels.jl by Matthieu Gomez, with additional features and improvements.","category":"section"},{"location":"ols.html#OLS-Estimation","page":"OLS Estimation","title":"OLS Estimation","text":"","category":"section"},{"location":"ols.html#Basic-OLS","page":"OLS Estimation","title":"Basic OLS","text":"The ols() function estimates linear models using Ordinary Least Squares:\n\nmodel = ols(df, @formula(y ~ x1 + x2))","category":"section"},{"location":"ols.html#Return-Type","page":"OLS Estimation","title":"Return Type","text":"Returns an OLSEstimator{T, P, V} where:\n\nT is the numeric type (Float64 or Float32)\nP is the predictor type (Cholesky or QR factorization)\nV is the variance estimator type","category":"section"},{"location":"ols.html#Accessing-Results","page":"OLS Estimation","title":"Accessing Results","text":"coef(model)          # Coefficient vector\nstderror(model)      # Standard errors (default HC1)\nvcov(model)          # Variance-covariance matrix\nconfint(model)       # 95% confidence intervals\ncoeftable(model)     # Full coefficient table with statistics\n\nnobs(model)          # Number of observations\ndof(model)           # Degrees of freedom (model)\ndof_residual(model)  # Degrees of freedom (residual)\nr2(model)            # R-squared\nadjr2(model)         # Adjusted R-squared\n\nresiduals(model)     # Residual vector (if saved)\nfitted(model)        # Fitted values (if saved)","category":"section"},{"location":"ols.html#Weighted-Regression","page":"OLS Estimation","title":"Weighted Regression","text":"Specify weights using a column name:\n\nmodel = ols(df, @formula(y ~ x), weights = :pop)","category":"section"},{"location":"ols.html#Categorical-Variables","page":"OLS Estimation","title":"Categorical Variables","text":"Categorical variables are automatically handled with appropriate dummy coding:\n\nusing CategoricalArrays\ndf.region = categorical(df.region)\n\nmodel = ols(df, @formula(y ~ x + region))\n\nControl contrast coding with the contrasts argument:\n\nmodel = ols(df, @formula(y ~ x + region),\n    contrasts = Dict(:region => DummyCoding(base = \"North\"))\n)","category":"section"},{"location":"ols.html#Prediction","page":"OLS Estimation","title":"Prediction","text":"","category":"section"},{"location":"ols.html#In-Sample-Prediction","page":"OLS Estimation","title":"In-Sample Prediction","text":"# Using stored fitted values (requires save = :residuals or :all)\nfitted(model)\n\n# Recompute predictions\npredict(model, df)","category":"section"},{"location":"ols.html#Out-of-Sample-Prediction","page":"OLS Estimation","title":"Out-of-Sample Prediction","text":"new_data = DataFrame(x1 = [1.0, 2.0], x2 = [0.5, 1.5])\npredict(model, new_data)","category":"section"},{"location":"ols.html#Collinearity-Handling","page":"OLS Estimation","title":"Collinearity Handling","text":"Regress.jl automatically detects and handles collinear variables:\n\n# x2 is perfectly collinear with x1\ndf.x2 = 2 * df.x1\n\nmodel = ols(df, @formula(y ~ x1 + x2))\n# Coefficient for x2 will be 0 with NaN standard error\n\nCheck which coefficients are identified:\n\nmodel.basis_coef  # BitVector indicating non-collinear coefficients","category":"section"},{"location":"ols.html#Solver-Options","page":"OLS Estimation","title":"Solver Options","text":"","category":"section"},{"location":"ols.html#Cholesky-vs-QR","page":"OLS Estimation","title":"Cholesky vs QR","text":"By default, Regress.jl uses Cholesky factorization for speed. For numerically ill-conditioned problems, QR factorization is more stable:\n\n# Automatic choice based on condition number\nmodel = ols(df, @formula(y ~ x))\n\n# The predictor type is stored in the model\nmodel.predictor  # OLSPredictorChol or OLSPredictorQR","category":"section"},{"location":"ols.html#Example","page":"OLS Estimation","title":"Example","text":"using Regress, DataFrames, CategoricalArrays\n\n# Load data\ndf = DataFrame(\n    sales = randn(1000) .+ 100,\n    price = randn(1000) .+ 50,\n    advertising = randn(1000) .+ 20,\n    region = rand([\"North\", \"South\", \"East\", \"West\"], 1000),\n    pop = rand(1000:5000, 1000)\n)\ndf.region = categorical(df.region)\n\n# Basic OLS\nm1 = ols(df, @formula(sales ~ price + advertising))\ncoeftable(m1)\n\n# With categorical variable\nm2 = ols(df, @formula(sales ~ price + advertising + region))\ncoeftable(m2)\n\n# Weighted regression\nm3 = ols(df, @formula(sales ~ price + advertising), weights = :pop)\ncoeftable(m3)\n\n# Robust standard errors\nm3_hc3 = m3 + vcov(HC3())\ncoeftable(m3_hc3)","category":"section"},{"location":"variance.html#Variance-Estimation","page":"Variance Estimation","title":"Variance Estimation","text":"Regress.jl integrates with CovarianceMatrices.jl for robust variance-covariance estimation.","category":"section"},{"location":"variance.html#The-model-vcov(...)-Syntax","page":"Variance Estimation","title":"The model + vcov(...) Syntax","text":"A convenient operator syntax allows updating a model's variance estimator:\n\nmodel = ols(df, @formula(y ~ x1 + x2))\n\n# Create a new model with HC3 standard errors\nmodel_hc3 = model + vcov(HC3())\n\n# Access updated statistics\nstderror(model_hc3)      # HC3 standard errors\ncoeftable(model_hc3)     # Coefficient table with HC3 inference\nmodel_hc3.F              # Robust Wald F-statistic\nmodel_hc3.p              # p-value of F-statistic\n\nThe + operator returns a wrapper containing:\n\nThe original model\nPrecomputed vcov matrix, standard errors, t-statistics, and p-values\nRobust Wald F-statistic for joint significance","category":"section"},{"location":"variance.html#Heteroskedasticity-Robust-Estimators","page":"Variance Estimation","title":"Heteroskedasticity-Robust Estimators","text":"","category":"section"},{"location":"variance.html#Available-HC-Estimators","page":"Variance Estimation","title":"Available HC Estimators","text":"Estimator Description\nHC0() White's (1980) estimator, no degrees of freedom adjustment\nHC1() DOF-adjusted (n/(n-k)), default in many packages\nHC2() Leverage-adjusted\nHC3() Jackknife-like, squared leverage adjustment\nHC4() Cribari-Neto (2004)\nHC5() Cribari-Neto modified","category":"section"},{"location":"variance.html#Usage","page":"Variance Estimation","title":"Usage","text":"# Via + operator\nmodel_hc3 = model + vcov(HC3())\n\n# Direct computation\nV = vcov(HC3(), model)\nse = stderror(HC3(), model)","category":"section"},{"location":"variance.html#Cluster-Robust-Estimators","page":"Variance Estimation","title":"Cluster-Robust Estimators","text":"Cluster-robust variance estimation accounts for within-cluster correlation.","category":"section"},{"location":"variance.html#Setup","page":"Variance Estimation","title":"Setup","text":"Save cluster variables during estimation:\n\nmodel = ols(df, @formula(y ~ x), save_cluster = :firm)\n\nFor multi-way clustering:\n\nmodel = ols(df, @formula(y ~ x), save_cluster = (:firm, :year))","category":"section"},{"location":"variance.html#Available-CR-Estimators","page":"Variance Estimation","title":"Available CR Estimators","text":"Estimator Description\nCR0() No small-sample adjustment\nCR1() G/(G-1) adjustment (Stata/R default)\nCR2() Bell-McCaffrey leverage adjustment\nCR3() Squared leverage adjustment","category":"section"},{"location":"variance.html#Usage-2","page":"Variance Estimation","title":"Usage","text":"# One-way clustering\nmodel_cr1 = model + vcov(CR1(:firm))\n\n# Two-way clustering\nmodel_cr12 = model + vcov(CR1(:firm, :year))\n\n# Direct computation\nV = vcov(CR1(:firm), model)\nse = stderror(CR1(:firm), model)","category":"section"},{"location":"variance.html#Small-Sample-Correction","page":"Variance Estimation","title":"Small-Sample Correction","text":"The default small-sample correction follows R fixest's formula:\n\nScale: G/(G-1) * (n-1)/(n-K)\nFixed effects nested in clusters are not counted in K","category":"section"},{"location":"variance.html#HAC-Estimators","page":"Variance Estimation","title":"HAC Estimators","text":"Heteroskedasticity and Autocorrelation Consistent (HAC) estimators for time series:","category":"section"},{"location":"variance.html#Available-HAC-Estimators","page":"Variance Estimation","title":"Available HAC Estimators","text":"Estimator Kernel\nBartlett(bw) Bartlett (Newey-West)\nParzen(bw) Parzen\nQuadraticSpectral(bw) Quadratic Spectral (Andrews)\nTukeyHanning(bw) Tukey-Hanning\nTruncated(bw) Truncated\n\nThe bw parameter is the bandwidth (number of lags).","category":"section"},{"location":"variance.html#Usage-3","page":"Variance Estimation","title":"Usage","text":"# Bartlett kernel with 5 lags (Newey-West)\nmodel_hac = model + vcov(Bartlett(5))\n\n# Quadratic spectral kernel\nmodel_qs = model + vcov(QuadraticSpectral(10))","category":"section"},{"location":"variance.html#Combining-with-IV-Models","page":"Variance Estimation","title":"Combining with IV Models","text":"All variance estimators work with IV models. The + vcov(...) syntax recomputes first-stage diagnostics:\n\nmodel = iv(TSLS(), df, @formula(y ~ x + (endo ~ z1 + z2)))\n\n# HC3 for IV\nmodel_hc3 = model + vcov(HC3())\n\n# First-stage F-stats are recomputed with HC3\nmodel_hc3.F_kp           # Joint first-stage F with HC3\nmodel_hc3.F_kp_per_endo  # Per-endogenous F-stats with HC3","category":"section"},{"location":"variance.html#Robust-Wald-F-Statistic","page":"Variance Estimation","title":"Robust Wald F-Statistic","text":"The Wald F-statistic tests the joint null hypothesis that all non-intercept coefficients are zero:\n\nmodel_hc3 = model + vcov(HC3())\n\nmodel_hc3.F   # Robust Wald F-statistic\nmodel_hc3.p   # p-value\n\nThe F-statistic is computed as:\n\nF = (1/q) * beta' * inv(V[non-intercept, non-intercept]) * beta\n\nwhere q is the number of non-intercept coefficients.","category":"section"},{"location":"variance.html#Performance-Considerations","page":"Variance Estimation","title":"Performance Considerations","text":"vcov() is computed lazily when called on a model\nThe + vcov(...) syntax precomputes and caches all statistics\nFor multiple variance estimators, use direct vcov(estimator, model) calls\nHAC estimators are more expensive than HC estimators","category":"section"},{"location":"variance.html#Example","page":"Variance Estimation","title":"Example","text":"using Regress, DataFrames\n\n# Create data\nn = 1000\ndf = DataFrame(\n    y = randn(n),\n    x1 = randn(n),\n    x2 = randn(n),\n    firm = rand(1:50, n),\n    year = rand(2010:2020, n)\n)\n\n# Fit model with cluster variable\nmodel = ols(df, @formula(y ~ x1 + x2), save_cluster = (:firm, :year))\n\n# Compare variance estimators\nprintln(\"HC1: \", stderror(model))\nprintln(\"HC3: \", stderror(model + vcov(HC3())))\nprintln(\"CR1 (firm): \", stderror(model + vcov(CR1(:firm))))\nprintln(\"CR1 (two-way): \", stderror(model + vcov(CR1(:firm, :year))))\n\n# Full inference with cluster-robust SE\nmodel_cr = model + vcov(CR1(:firm))\ncoeftable(model_cr)\nprintln(\"Robust Wald F: \", model_cr.F)","category":"section"}]
}
